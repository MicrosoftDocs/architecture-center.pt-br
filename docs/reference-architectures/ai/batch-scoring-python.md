---
title: Pontuação em lote de modelos Python no Azure
description: Compile uma solução escalonável para modelos de pontuação em lote com agendamento em paralelo usando o IA do Lote do Azure.
author: njray
ms.date: 12/13/2018
ms.topic: reference-architecture
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: azcat-ai, AI
ms.openlocfilehash: a291821860a8e503ba4c6173ac6d8fd449d6ebf3
ms.sourcegitcommit: 1b50810208354577b00e89e5c031b774b02736e2
ms.translationtype: HT
ms.contentlocale: pt-BR
ms.lasthandoff: 01/23/2019
ms.locfileid: "54485359"
---
# <a name="batch-scoring-of-python-models-on-azure"></a><span data-ttu-id="febda-103">Pontuação em lote de modelos Python no Azure</span><span class="sxs-lookup"><span data-stu-id="febda-103">Batch scoring of Python models on Azure</span></span>

<span data-ttu-id="febda-104">Esta arquitetura de referência mostra como compilar uma solução escalonável para pontuar vários modelos em lote com agendamento em paralelo usando o IA do Lote do Azure.</span><span class="sxs-lookup"><span data-stu-id="febda-104">This reference architecture shows how to build a scalable solution for batch scoring many models on a schedule in parallel using Azure Batch AI.</span></span> <span data-ttu-id="febda-105">A solução pode ser usada como um modelo e pode ser generalizada para problemas diferentes.</span><span class="sxs-lookup"><span data-stu-id="febda-105">The solution can be used as a template and can generalize to different problems.</span></span>

<span data-ttu-id="febda-106">Há uma implantação de referência para essa arquitetura disponível no  [GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="febda-106">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Pontuação em lote de modelos Python no Azure](./_images/batch-scoring-python.png)

<span data-ttu-id="febda-108">**Cenário**: A solução monitora a operação de um grande número de dispositivos em uma configuração de IoT, em que cada dispositivo envia leituras do sensor continuamente.</span><span class="sxs-lookup"><span data-stu-id="febda-108">**Scenario**: This solution monitors the operation of a large number of devices in an IoT setting where each device sends sensor readings continuously.</span></span> <span data-ttu-id="febda-109">Assume-se que cada dispositivo tem modelos de detecção de anomalias pré-treinados que precisam ser usados para prever se uma série de medidas, agregadas em um intervalo de tempo predefinido, correspondem a uma anomalia ou não.</span><span class="sxs-lookup"><span data-stu-id="febda-109">Each device is assumed to have pre-trained anomaly detection models that need to be used to predict whether a series of measurements, that are aggregated over a predefined time interval, correspond to an anomaly or not.</span></span> <span data-ttu-id="febda-110">Em cenários do mundo real, isso seria um fluxo de leituras de sensor que precisariam ser filtrados e agregados antes de serem usados em treinamento ou pontuação em tempo real.</span><span class="sxs-lookup"><span data-stu-id="febda-110">In real-world scenarios, this could be a stream of sensor readings that need to be filtered and aggregated before being used in training or real-time scoring.</span></span> <span data-ttu-id="febda-111">Para manter a simplicidade, a solução usa o mesmo arquivo de dados na execução de trabalhos de pontuação.</span><span class="sxs-lookup"><span data-stu-id="febda-111">For simplicity, the solution uses the same data file when executing scoring jobs.</span></span>

## <a name="architecture"></a><span data-ttu-id="febda-112">Arquitetura</span><span class="sxs-lookup"><span data-stu-id="febda-112">Architecture</span></span>

<span data-ttu-id="febda-113">Essa arquitetura é formada pelos seguintes componentes:</span><span class="sxs-lookup"><span data-stu-id="febda-113">This architecture consists of the following components:</span></span>

<span data-ttu-id="febda-114">[Hubs de Eventos do Azure][event-hubs].</span><span class="sxs-lookup"><span data-stu-id="febda-114">[Azure Event Hubs][event-hubs].</span></span> <span data-ttu-id="febda-115">Esse serviço de ingestão de mensagens pode incluir milhões de mensagens de eventos por segundo.</span><span class="sxs-lookup"><span data-stu-id="febda-115">This message ingestion service can ingest millions of event messages per second.</span></span> <span data-ttu-id="febda-116">Nesta arquitetura, os sensores enviam um fluxo de dados para o hub de eventos.</span><span class="sxs-lookup"><span data-stu-id="febda-116">In this architecture, sensors send a stream of data to the event hub.</span></span>

<span data-ttu-id="febda-117">[Azure Stream Analytics][stream-analytics].</span><span class="sxs-lookup"><span data-stu-id="febda-117">[Azure Stream Analytics][stream-analytics].</span></span> <span data-ttu-id="febda-118">Um mecanismo de processamento de eventos.</span><span class="sxs-lookup"><span data-stu-id="febda-118">An event-processing engine.</span></span> <span data-ttu-id="febda-119">Um trabalho do Stream Analytics lê os fluxos de dados do hub de eventos e executa o processamento de fluxo.</span><span class="sxs-lookup"><span data-stu-id="febda-119">A Stream Analytics job reads the data streams from the event hub and performs stream processing.</span></span>

<span data-ttu-id="febda-120">[IA do Lote do Azure][batch-ai].</span><span class="sxs-lookup"><span data-stu-id="febda-120">[Azure Batch AI][batch-ai].</span></span> <span data-ttu-id="febda-121">Esse mecanismo de computação distribuída é usado para treinar e testar modelos de machine learning e de inteligência artificial em larga escala no Azure.</span><span class="sxs-lookup"><span data-stu-id="febda-121">This distributed computing engine is used to train and test machine learning and AI models at scale in Azure.</span></span> <span data-ttu-id="febda-122">A IA do Lote cria máquinas virtuais sob demanda com uma opção de dimensionamento automático, em que cada nó no cluster do IA do Lote executa um trabalho de pontuação para um sensor específico.</span><span class="sxs-lookup"><span data-stu-id="febda-122">Batch AI creates virtual machines on demand with an automatic scaling option, where each node in the Batch AI cluster runs a scoring job for a specific sensor.</span></span> <span data-ttu-id="febda-123">O  [script][python-script] de pontuação em Python é executado em contêineres do Docker que são criados em cada nó do cluster, onde ele lê os dados de sensor relevantes, gera previsões e as armazena no Armazenamento de Blobs.</span><span class="sxs-lookup"><span data-stu-id="febda-123">The scoring Python [script][python-script] runs in Docker containers that are created on each node of the cluster, where it reads the relevant sensor data, generates predictions and stores them in Blob storage.</span></span>

<span data-ttu-id="febda-124">[Armazenamento de Blobs do Azure][storage].</span><span class="sxs-lookup"><span data-stu-id="febda-124">[Azure Blob Storage][storage].</span></span> <span data-ttu-id="febda-125">Os contêineres de blob são usados para armazenar os modelos pré-treinados, os dados e as previsões de saída.</span><span class="sxs-lookup"><span data-stu-id="febda-125">Blob containers are used to store the pretrained models, the data, and the output predictions.</span></span> <span data-ttu-id="febda-126">Os modelos são carregados no Armazenamento de Blobs no notebook [create\_resources.ipynb] [ create-resources].</span><span class="sxs-lookup"><span data-stu-id="febda-126">The models are uploaded to Blob storage in the [create\_resources.ipynb][create-resources] notebook.</span></span> <span data-ttu-id="febda-127">Esses modelos de [SVM de classe única][one-class-svm] são treinados com dados que representam valores de sensores diferentes em dispositivos diferentes.</span><span class="sxs-lookup"><span data-stu-id="febda-127">These [one-class SVM][one-class-svm] models are trained on data that represents values of different sensors for different devices.</span></span> <span data-ttu-id="febda-128">A solução pressupõe que os valores de dados foram agregados em um intervalo fixo de tempo.</span><span class="sxs-lookup"><span data-stu-id="febda-128">This solution assumes that the data values are aggregated over a fixed interval of time.</span></span>

<span data-ttu-id="febda-129">[Aplicativos Lógicos do Azure][logic-apps].</span><span class="sxs-lookup"><span data-stu-id="febda-129">[Azure Logic Apps][logic-apps].</span></span> <span data-ttu-id="febda-130">Esta solução cria um Aplicativo Lógico que executa trabalhos da IA do Lote a cada hora.</span><span class="sxs-lookup"><span data-stu-id="febda-130">This solution creates a Logic App that runs hourly Batch AI jobs.</span></span> <span data-ttu-id="febda-131">Os Aplicativos Lógicos fornecem uma maneira fácil de criar o fluxo de trabalho de tempo de execução e o agendamento para a solução.</span><span class="sxs-lookup"><span data-stu-id="febda-131">Logic Apps provides an easy way to create the runtime workflow and scheduling for the solution.</span></span> <span data-ttu-id="febda-132">Os trabalhos da IA do Lote são enviados usando um [script][script] em Python que também é executado em um contêiner do Docker.</span><span class="sxs-lookup"><span data-stu-id="febda-132">The Batch AI jobs are submitted using a Python [script][script] that also runs in a Docker container.</span></span>

<span data-ttu-id="febda-133">[Registro de Contêiner do Azure][acr].</span><span class="sxs-lookup"><span data-stu-id="febda-133">[Azure Container Registry][acr].</span></span> <span data-ttu-id="febda-134">As Imagens do Docker são usadas nos Aplicativos Lógicos e na IA do Lote, e são criadas no notebook [create\_resources.ipynb][create-resources]; em seguida, são enviadas por push ao Registro de Contêiner.</span><span class="sxs-lookup"><span data-stu-id="febda-134">Docker images are used in both Batch AI and Logic Apps and are created in the [create\_resources.ipynb][create-resources] notebook, then pushed to Container Registry.</span></span> <span data-ttu-id="febda-135">Isso fornece uma maneira conveniente para hospedar imagens e criar instâncias de contêineres por meio de outros serviços do Azure; no caso desta solução, os Aplicativos Lógicos e a IA do Lote.</span><span class="sxs-lookup"><span data-stu-id="febda-135">This provides a convenient way to host images and instantiate containers through other Azure services—Logic Apps and Batch AI in this solution.</span></span>

## <a name="performance-considerations"></a><span data-ttu-id="febda-136">Considerações sobre o desempenho</span><span class="sxs-lookup"><span data-stu-id="febda-136">Performance considerations</span></span>

<span data-ttu-id="febda-137">Para modelos Python normais, é geralmente aceito que as CPUs são suficientes para lidar com a carga de trabalho.</span><span class="sxs-lookup"><span data-stu-id="febda-137">For standard Python models, it's generally accepted that CPUs are sufficient to handle the workload.</span></span> <span data-ttu-id="febda-138">Esta arquitetura usa CPUs.</span><span class="sxs-lookup"><span data-stu-id="febda-138">This architecture uses CPUs.</span></span> <span data-ttu-id="febda-139">No entanto, para [cargas de trabalho de aprendizado profundo][deep], as GPUs geralmente superam bastante as CPUs; elas costumam exigir um cluster considerável de CPUs para obter um desempenho comparável.</span><span class="sxs-lookup"><span data-stu-id="febda-139">However, for [deep learning workloads][deep], GPUs generally outperform CPUs by a considerable amount—a sizeable cluster of CPUs is usually needed to get comparable performance.</span></span>

### <a name="parallelizing-across-vms-vs-cores"></a><span data-ttu-id="febda-140">Paralelização em VMs versus núcleos</span><span class="sxs-lookup"><span data-stu-id="febda-140">Parallelizing across VMs vs cores</span></span>

<span data-ttu-id="febda-141">Ao executar processos de pontuação de vários modelos em modo de lote, os trabalhos precisam ser colocados em paralelo entre VMs.</span><span class="sxs-lookup"><span data-stu-id="febda-141">When running scoring processes of many models in batch mode, the jobs need to be parallelized across VMs.</span></span> <span data-ttu-id="febda-142">Há duas abordagens possíveis:</span><span class="sxs-lookup"><span data-stu-id="febda-142">Two approaches are possible:</span></span>

* <span data-ttu-id="febda-143">Criar um cluster maior usando VMs de baixo custo.</span><span class="sxs-lookup"><span data-stu-id="febda-143">Create a larger cluster using low-cost VMs.</span></span>

* <span data-ttu-id="febda-144">Criar um cluster menor usando VMs de alto desempenho com mais núcleos disponíveis em cada uma.</span><span class="sxs-lookup"><span data-stu-id="febda-144">Create a smaller cluster using high performing VMs with more cores available on each.</span></span>

<span data-ttu-id="febda-145">Em geral, a pontuação de modelos Python padrão não é tão exigente quanto a pontuação de modelos de aprendizado profundo, e um pequeno cluster deve ser capaz de lidar com um grande número de modelos em fila eficientemente.</span><span class="sxs-lookup"><span data-stu-id="febda-145">In general, scoring of standard Python models is not as demanding as scoring of deep learning models, and a small cluster should be able to handle a large number of queued models efficiently.</span></span> <span data-ttu-id="febda-146">Você pode aumentar o número de nós de cluster de acordo com o aumento dos tamanhos do conjunto de dados.</span><span class="sxs-lookup"><span data-stu-id="febda-146">You can increase the number of cluster nodes as the dataset sizes increase.</span></span>

<span data-ttu-id="febda-147">Por questões de conveniência neste cenário, uma tarefa de pontuação é enviada dentro de um único trabalho da IA do Lote.</span><span class="sxs-lookup"><span data-stu-id="febda-147">For convenience in this scenario, one scoring task is submitted within a single Batch AI job.</span></span> <span data-ttu-id="febda-148">No entanto, pode ser mais eficiente pontuar várias partes de dados dentro do mesmo trabalho da IA do Lote.</span><span class="sxs-lookup"><span data-stu-id="febda-148">However, it can be more efficient to score multiple data chunks within the same Batch AI job.</span></span> <span data-ttu-id="febda-149">Nesses casos, escreva código personalizado para ler vários conjuntos de dados e executar o script de pontuação para eles durante uma mesma execução de trabalho da IA do Lote.</span><span class="sxs-lookup"><span data-stu-id="febda-149">In those cases, write custom code to read in multiple datasets and execute the scoring script for those during a single Batch AI job execution.</span></span>

### <a name="file-servers"></a><span data-ttu-id="febda-150">Servidores de arquivos</span><span class="sxs-lookup"><span data-stu-id="febda-150">File servers</span></span>

<span data-ttu-id="febda-151">Ao usar a IA do Lote, você pode escolher várias opções de armazenamento, dependendo da taxa de transferência necessária para o seu cenário.</span><span class="sxs-lookup"><span data-stu-id="febda-151">When using Batch AI, you can choose multiple storage options depending on the throughput needed for your scenario.</span></span> <span data-ttu-id="febda-152">Para cargas de trabalho que precisam de uma taxa de transferência baixa, o uso do armazenamento de blobs deve ser suficiente.</span><span class="sxs-lookup"><span data-stu-id="febda-152">For workloads with low throughput requirements, using blob storage should be enough.</span></span> <span data-ttu-id="febda-153">Como alternativa, a IA do Lote também dá suporte a um [Servidor de Arquivos da IA do Lote][bai-file-server], um NFS gerenciado de nó único, que pode ser montado automaticamente em nós de cluster para fornecer um local de armazenamento de acesso centralizado para trabalhos.</span><span class="sxs-lookup"><span data-stu-id="febda-153">Alternatively, Batch AI also supports a [Batch AI File Server][bai-file-server], a managed, single-node NFS, which can be automatically mounted on cluster nodes to provide a centrally accessible storage location for jobs.</span></span> <span data-ttu-id="febda-154">Na maioria dos casos, somente um servidor de arquivos é necessário em um espaço de trabalho e você pode separar os dados de seus trabalhos de treinamento em diretórios diferentes.</span><span class="sxs-lookup"><span data-stu-id="febda-154">For most cases, only one file server is needed in a workspace, and you can separate data for your training jobs into different directories.</span></span>

<span data-ttu-id="febda-155">Se o NFS de nó único não for adequado para suas cargas de trabalho, a IA do Lote dá suporte a outras opções de armazenamento, incluindo o [Azure Files][azure-files] e soluções personalizadas, como um sistema de arquivos Gluster ou Lustre.</span><span class="sxs-lookup"><span data-stu-id="febda-155">If a single-node NFS isn't appropriate for your workloads, Batch AI supports other storage options, including [Azure Files][azure-files] and custom solutions such as a Gluster or Lustre file system.</span></span>

## <a name="management-considerations"></a><span data-ttu-id="febda-156">Considerações de gerenciamento</span><span class="sxs-lookup"><span data-stu-id="febda-156">Management considerations</span></span>

### <a name="monitoring-batch-ai-jobs"></a><span data-ttu-id="febda-157">Monitoramento de trabalhos da IA do Lote</span><span class="sxs-lookup"><span data-stu-id="febda-157">Monitoring Batch AI jobs</span></span>

<span data-ttu-id="febda-158">É importante monitorar o progresso de trabalhos em execução, mas pode ser um desafio monitorá-los em um cluster de nós ativos.</span><span class="sxs-lookup"><span data-stu-id="febda-158">It's important to monitor the progress of running jobs, but it can be a challenge to monitor across a cluster of active nodes.</span></span> <span data-ttu-id="febda-159">Para ter uma ideia do estado geral do cluster, vá até a folha de **IA do Lote** do [Portal do Azure][portal] para inspecionar o estado dos nós no cluster.</span><span class="sxs-lookup"><span data-stu-id="febda-159">To get a sense of the overall state of the cluster, go to the **Batch AI** blade of the [Azure Portal][portal] to inspect the state of the nodes in the cluster.</span></span> <span data-ttu-id="febda-160">Se um nó estiver inativo ou se um trabalho falhar, os logs de erro serão salvos no armazenamento de blobs e também poderão ser acessados na folha **Trabalhos** no portal.</span><span class="sxs-lookup"><span data-stu-id="febda-160">If a node is inactive or a job has failed, the error logs are saved to blob storage, and are also accessible in the **Jobs** blade of the portal.</span></span>

<span data-ttu-id="febda-161">Para um monitoramento mais avançado, conecte os logs ao [Application Insights][ai], ou execute processos separados para sondar o estado do cluster da IA do Lote e de seus trabalhos.</span><span class="sxs-lookup"><span data-stu-id="febda-161">For richer monitoring, connect logs to [Application Insights][ai], or run separate processes to poll for the state of the Batch AI cluster and its jobs.</span></span>

### <a name="logging-in-batch-ai"></a><span data-ttu-id="febda-162">Registro em log na IA do Lote</span><span class="sxs-lookup"><span data-stu-id="febda-162">Logging in Batch AI</span></span>

<span data-ttu-id="febda-163">A IA do Lote registra todos os stdout/stderr da conta do Armazenamento do Azure associada.</span><span class="sxs-lookup"><span data-stu-id="febda-163">Batch AI logs all stdout/stderr to the associated Azure storage account.</span></span> <span data-ttu-id="febda-164">Para facilitar a navegação dos arquivos de log, use uma ferramenta de navegação de armazenamento, como o [Gerenciador de Armazenamento do Azure][explorer].</span><span class="sxs-lookup"><span data-stu-id="febda-164">For easy navigation of the log files, use a storage navigation tool such as [Azure Storage Explorer][explorer].</span></span>

<span data-ttu-id="febda-165">Quando você implanta esta arquitetura de referência, tem a opção de configurar um sistema de registro em log mais simples.</span><span class="sxs-lookup"><span data-stu-id="febda-165">When you deploy this reference architecture, you have the option to set up a simpler logging system.</span></span> <span data-ttu-id="febda-166">Com essa opção, todos os logs nos diferentes trabalhos são salvos no mesmo diretório em seu contêiner de blobs, conforme mostrado abaixo.</span><span class="sxs-lookup"><span data-stu-id="febda-166">With this option, all the logs across the different jobs are saved to the same directory in your blob container as shown below.</span></span> <span data-ttu-id="febda-167">Use esses logs para monitorar quanto tempo demora para processar cada imagem e cada trabalho, para que você tenha uma noção melhor de como otimizar o processo.</span><span class="sxs-lookup"><span data-stu-id="febda-167">Use these logs to monitor how long it takes for each job and each image to process, so you have a better sense of how to optimize the process.</span></span>

![Gerenciador de Armazenamento do Azure](./_images/batch-scoring-python-monitor.png)

## <a name="cost-considerations"></a><span data-ttu-id="febda-169">Considerações de custo</span><span class="sxs-lookup"><span data-stu-id="febda-169">Cost considerations</span></span>

<span data-ttu-id="febda-170">Os componentes mais caros usados nesta arquitetura de referência são os recursos de computação.</span><span class="sxs-lookup"><span data-stu-id="febda-170">The most expensive components used in this reference architecture are the compute resources.</span></span>

<span data-ttu-id="febda-171">O tamanho do cluster da IA do Lote é escalado e reduzido verticalmente dependendo dos trabalhos na fila.</span><span class="sxs-lookup"><span data-stu-id="febda-171">The Batch AI cluster size scales up and down depending on the jobs in the queue.</span></span> <span data-ttu-id="febda-172">Você pode habilitar o [dimensionamento automático][automatic-scaling] com a IA do Lote das duas maneiras.</span><span class="sxs-lookup"><span data-stu-id="febda-172">You can enable [automatic scaling][automatic-scaling] with Batch AI in one of two ways.</span></span> <span data-ttu-id="febda-173">É possível fazer isso programaticamente, fazendo a configuração no arquivo .env que faz parte das [etapas de implantação][github], ou pela alteração da fórmula de dimensionamento diretamente no portal após a criação do cluster.</span><span class="sxs-lookup"><span data-stu-id="febda-173">You can do so programmatically, which can be configured in the .env file that is part of the [deployment steps][github], or you can change the scale formula directly in the portal after the cluster is created.</span></span>

<span data-ttu-id="febda-174">Para trabalho que não exige processamento imediato, configure a fórmula de dimensionamento automático a fim de definir o estado padrão (mínimo) como um cluster sem nós.</span><span class="sxs-lookup"><span data-stu-id="febda-174">For work that doesn't require immediate processing, configure the automatic scaling formula so the default state (minimum) is a cluster of zero nodes.</span></span> <span data-ttu-id="febda-175">Com essa configuração, o cluster começa com zero nós e só pode ser escalado verticalmente quando detecta os trabalhos na fila.</span><span class="sxs-lookup"><span data-stu-id="febda-175">With this configuration, the cluster starts with zero nodes and only scales up when it detects jobs in the queue.</span></span> <span data-ttu-id="febda-176">Se o processo de pontuação do lote acontecer apenas algumas vezes por dia, essa configuração permitirá uma economia considerável.</span><span class="sxs-lookup"><span data-stu-id="febda-176">If the batch scoring process only happens a few times a day or less, this setting enables significant cost savings.</span></span>

<span data-ttu-id="febda-177">Talvez o dimensionamento automático não seja apropriado para trabalhos em lote que aconteçam muito próximos uns dos outros.</span><span class="sxs-lookup"><span data-stu-id="febda-177">Automatic scaling may not be appropriate for batch jobs that happen too close to each other.</span></span> <span data-ttu-id="febda-178">O tempo de ativação e desativação de um cluster também incorre em um custo, portanto, se uma carga de trabalho do lote começar apenas alguns minutos após o término do trabalho anterior, talvez seja mais econômico manter o cluster em execução entre os trabalhos.</span><span class="sxs-lookup"><span data-stu-id="febda-178">The time that it takes for a cluster to spin up and spin down also incur a cost, so if a batch workload begins only a few minutes after the previous job ends, it might be more cost effective to keep the cluster running between jobs.</span></span> <span data-ttu-id="febda-179">Isso depende da frequência de agendamento dos processos de pontuação, com alta frequência (a cada hora, por exemplo) ou com baixa frequência (uma vez por mês, por exemplo).</span><span class="sxs-lookup"><span data-stu-id="febda-179">That depends on whether scoring processes are scheduled to run at a high frequency (every hour, for example), or less frequently (once a month, for example).</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="febda-180">Implantar a solução</span><span class="sxs-lookup"><span data-stu-id="febda-180">Deploy the solution</span></span>

<span data-ttu-id="febda-181">Há uma implantação de referência para essa arquitetura disponível no [GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="febda-181">The reference implementation of this architecture is available on [GitHub][github].</span></span> <span data-ttu-id="febda-182">Siga as etapas de configuração para criar uma solução escalonável e pontuar muitos modelos em paralelo usando a IA do Lote.</span><span class="sxs-lookup"><span data-stu-id="febda-182">Follow the setup steps there to build a scalable solution for scoring many models in parallel using Batch AI.</span></span>

[acr]: /azure/container-registry/container-registry-intro
[ai]: /azure/application-insights/app-insights-overview
[automatic-scaling]: /azure/batch/batch-automatic-scaling
[azure-files]: /azure/storage/files/storage-files-introduction
[batch-ai]: /azure/batch-ai/
[bai-file-server]: /azure/batch-ai/resource-concepts#file-server
[create-resources]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/create_resources.ipynb
[deep]: /azure/architecture/reference-architectures/ai/batch-scoring-deep-learning
[event-hubs]: /azure/event-hubs/event-hubs-geo-dr
[explorer]: https://azure.microsoft.com/en-us/features/storage-explorer/
[github]: https://github.com/Azure/BatchAIAnomalyDetection
[logic-apps]: /azure/logic-apps/logic-apps-overview
[one-class-svm]: http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html
[portal]: https://portal.azure.com
[python-script]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/batchai/predict.py
[script]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/sched/submit_jobs.py
[storage]: /azure/storage/blobs/storage-blobs-overview
[stream-analytics]: /azure/stream-analytics/
