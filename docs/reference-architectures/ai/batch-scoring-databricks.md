---
title: Pontuação em lote dos modelos do Spark no Azure Databricks
description: Crie uma solução escalonável para pontuar em lote um modelo de classificação do Apache Spark usando o Azure Databricks.
author: njray
ms.date: 02/07/2019
ms.topic: reference-architecture
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: azcat-ai
ms.openlocfilehash: cba8d272ddbdbf2c2da94f68b288e9fb79be7de2
ms.sourcegitcommit: 1a3cc91530d56731029ea091db1f15d41ac056af
ms.translationtype: MT
ms.contentlocale: pt-BR
ms.lasthandoff: 04/03/2019
ms.locfileid: "58887804"
---
# <a name="batch-scoring-of-spark-machine-learning-models-on-azure-databricks"></a><span data-ttu-id="da6fe-103">Modelos de pontuação do lote do machine learning do Spark no Azure Databricks</span><span class="sxs-lookup"><span data-stu-id="da6fe-103">Batch scoring of Spark machine learning models on Azure Databricks</span></span>

<span data-ttu-id="da6fe-104">Essa arquitetura de referência mostra como compilar uma solução escalonável para pontuação em lote no modelo de classificação do Apache Spark em um cronograma usando o Azure Databricks, uma plataforma analítica baseada no Apache Spark otimizada para o Azure.</span><span class="sxs-lookup"><span data-stu-id="da6fe-104">This reference architecture shows how to build a scalable solution for batch scoring an Apache Spark classification model on a schedule using Azure Databricks, an Apache Spark-based analytics platform optimized for Azure.</span></span> <span data-ttu-id="da6fe-105">A solução pode ser usada como um modelo que pode ser generalizado para outros cenários.</span><span class="sxs-lookup"><span data-stu-id="da6fe-105">The solution can be used as a template that can be generalized to other scenarios.</span></span>

<span data-ttu-id="da6fe-106">Há uma implantação de referência para essa arquitetura disponível no  [GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="da6fe-106">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Pontuação em lote dos modelos do Spark no Azure Databricks](./_images/batch-scoring-spark.png)

<span data-ttu-id="da6fe-108">**Cenário**: Uma empresa em um setor de ativos pesados deseja minimizar os custos e o tempo de inatividade associado a falhas mecânicas inesperadas.</span><span class="sxs-lookup"><span data-stu-id="da6fe-108">**Scenario**: A business in an asset-heavy industry wants to minimize the costs and downtime associated with unexpected mechanical failures.</span></span> <span data-ttu-id="da6fe-109">Usando dados de IoT coletadas dos seus computadores, podem criar um modelo de manutenção preditiva.</span><span class="sxs-lookup"><span data-stu-id="da6fe-109">Using IoT data collected from their machines, they can create a predictive maintenance model.</span></span> <span data-ttu-id="da6fe-110">Esse modelo permite que os negócios mantenham componentes de forma proativa e repara-os antes de falharem.</span><span class="sxs-lookup"><span data-stu-id="da6fe-110">This model enables the business to maintain components proactively and repair them before they fail.</span></span> <span data-ttu-id="da6fe-111">Ao maximizar o uso de componente mecânico, eles podem controlar os custos e reduzir o tempo de inatividade.</span><span class="sxs-lookup"><span data-stu-id="da6fe-111">By maximizing mechanical component use, they can control costs and reduce downtime.</span></span>

<span data-ttu-id="da6fe-112">Um modelo de manutenção preditiva coleta dados das máquinas e retém os exemplos de histórico de falhas de componente.</span><span class="sxs-lookup"><span data-stu-id="da6fe-112">A predictive maintenance model collects data from the machines and retains historical examples of component failures.</span></span> <span data-ttu-id="da6fe-113">O modelo, em seguida, pode ser usado para monitorar o estado atual dos componentes e prever se um determinado componente falhará em um futuro próximo.</span><span class="sxs-lookup"><span data-stu-id="da6fe-113">The model can then be used to monitor the current state of the components and predict if a given component will fail in the near future.</span></span> <span data-ttu-id="da6fe-114">Para casos de uso comum e abordagens de modelação, consulte [Guia de IA do Azure para soluções de manutenção preditiva][ai-guide].</span><span class="sxs-lookup"><span data-stu-id="da6fe-114">For common use cases and modeling approaches, see [Azure AI guide for predictive maintenance solutions][ai-guide].</span></span>

<span data-ttu-id="da6fe-115">Essa arquitetura de referência foi projetada para cargas de trabalho que são disparadas pela presença de novos dados de máquinas de componente.</span><span class="sxs-lookup"><span data-stu-id="da6fe-115">This reference architecture is designed for workloads that are triggered by the presence of new data from the component machines.</span></span> <span data-ttu-id="da6fe-116">O processamento envolve as seguintes etapas:</span><span class="sxs-lookup"><span data-stu-id="da6fe-116">Processing involves the following steps:</span></span>

1. <span data-ttu-id="da6fe-117">A ingestão de dados de armazenamento de dados externos no armazenamento de dados do Azure Databricks.</span><span class="sxs-lookup"><span data-stu-id="da6fe-117">Ingest the data from the external data store onto an Azure Databricks data store.</span></span>

2. <span data-ttu-id="da6fe-118">Treine um modelo de aprendizado de máquina transformando um conjunto de dados de treinamento, em seguida, crie um modelo do Spark MLlib.</span><span class="sxs-lookup"><span data-stu-id="da6fe-118">Train a machine learning model by transforming the data into a training data set, then building a Spark MLlib model.</span></span> <span data-ttu-id="da6fe-119">O MLlib consiste em algoritmos de aprendizado de máquina e utilitários otimizados para usufruir os recursos de escalabilidade de dados do Spark mais comuns.</span><span class="sxs-lookup"><span data-stu-id="da6fe-119">MLlib consists of most common machine learning algorithms and utilities optimized to take advantage of Spark data scalability capabilities.</span></span>

3. <span data-ttu-id="da6fe-120">Aplique o modelo treinado para prever (classificar) as falhas de componente, transformando os dados em um conjunto de dados de pontuação.</span><span class="sxs-lookup"><span data-stu-id="da6fe-120">Apply the trained model to predict (classify) component failures by transforming the data into a scoring data set.</span></span> <span data-ttu-id="da6fe-121">Pontue os dados com o modelo do Spark MLLib.</span><span class="sxs-lookup"><span data-stu-id="da6fe-121">Score the data with the Spark MLLib model.</span></span>

4. <span data-ttu-id="da6fe-122">Armazene resultados no armazenamento de dados de Databricks para consumo de pós-processamento.</span><span class="sxs-lookup"><span data-stu-id="da6fe-122">Store results on the Databricks data store for post-processing consumption.</span></span>

<span data-ttu-id="da6fe-123">Notebooks são fornecidos no [GitHub] [ github] para executar cada uma dessas tarefas.</span><span class="sxs-lookup"><span data-stu-id="da6fe-123">Notebooks are provided on [GitHub][github] to perform each of these tasks.</span></span>

## <a name="architecture"></a><span data-ttu-id="da6fe-124">Arquitetura</span><span class="sxs-lookup"><span data-stu-id="da6fe-124">Architecture</span></span>

<span data-ttu-id="da6fe-125">A arquitetura define um fluxo de dados que está inteiramente contido dentro do [Azure Databricks][databricks] com base em um conjunto de [notebooks][notebooks] executados sequencialmente.</span><span class="sxs-lookup"><span data-stu-id="da6fe-125">The architecture defines a data flow that is entirely contained within [Azure Databricks][databricks] based on a set of sequentially executed [notebooks][notebooks].</span></span> <span data-ttu-id="da6fe-126">Isso consiste nos seguintes componentes:</span><span class="sxs-lookup"><span data-stu-id="da6fe-126">It consists of the following components:</span></span>

<span data-ttu-id="da6fe-127">**[Arquivos de dados][github]**.</span><span class="sxs-lookup"><span data-stu-id="da6fe-127">**[Data files][github]**.</span></span> <span data-ttu-id="da6fe-128">A implementação de referência usa um conjunto de dados simulados contido em cinco arquivos de dados estáticos.</span><span class="sxs-lookup"><span data-stu-id="da6fe-128">The reference implementation uses a simulated data set contained in five static data files.</span></span>

<span data-ttu-id="da6fe-129">**[Ingestão][notebooks]**.</span><span class="sxs-lookup"><span data-stu-id="da6fe-129">**[Ingestion][notebooks]**.</span></span> <span data-ttu-id="da6fe-130">O notebook de ingestão de dados baixa os arquivos de dados de entrada em uma coleção de conjuntos de dados do Databricks.</span><span class="sxs-lookup"><span data-stu-id="da6fe-130">The data ingestion notebook downloads the input data files into a collection of Databricks data sets.</span></span> <span data-ttu-id="da6fe-131">Em um cenário do mundo real, dados de dispositivos IoT iriam para o armazenamento acessível de Databricks como Azure SQL Server ou Armazenamento de Blobs do Azure.</span><span class="sxs-lookup"><span data-stu-id="da6fe-131">In a real-world scenario, data from IoT devices would stream onto Databricks-accessible storage such as Azure SQL Server or Azure Blob storage.</span></span> <span data-ttu-id="da6fe-132">O Databricks dá suporte a várias [fontes de dados][data-sources].</span><span class="sxs-lookup"><span data-stu-id="da6fe-132">Databricks supports multiple [data sources][data-sources].</span></span>

<span data-ttu-id="da6fe-133">**Pipeline de treinamento**.</span><span class="sxs-lookup"><span data-stu-id="da6fe-133">**Training pipeline**.</span></span> <span data-ttu-id="da6fe-134">Este notebook executa o notebook de engenharia de recursos para criar um conjunto de dados de análise de dados ingeridos.</span><span class="sxs-lookup"><span data-stu-id="da6fe-134">This notebook executes the feature engineering notebook to create an analysis data set from the ingested data.</span></span> <span data-ttu-id="da6fe-135">Depois, executará um notebook de criação de modelo que treina o modelo de machine learning usando a biblioteca de aprendizado de máquina executa do [Apache Spark MLlib][mllib].</span><span class="sxs-lookup"><span data-stu-id="da6fe-135">It then executes a model building notebook that trains the machine learning model using the [Apache Spark MLlib][mllib] scalable machine learning library.</span></span>

<span data-ttu-id="da6fe-136">**Pipeline de pontuação**.</span><span class="sxs-lookup"><span data-stu-id="da6fe-136">**Scoring pipeline**.</span></span> <span data-ttu-id="da6fe-137">Este notebook executa o notebook de engenharia de recursos para criar o conjunto de dados de pontuação dos dados ingeridos e executa notebook de pontuação.</span><span class="sxs-lookup"><span data-stu-id="da6fe-137">This notebook executes the feature engineering notebook to create scoring data set from the ingested data and executes the scoring notebook.</span></span> <span data-ttu-id="da6fe-138">O notebook de pontuação usa o modelo [Spark MLlib][mllib-spark] para gerar previsões para as observações no conjunto de dados de pontuação.</span><span class="sxs-lookup"><span data-stu-id="da6fe-138">The scoring notebook uses the trained [Spark MLlib][mllib-spark] model to generate predictions for the observations in the scoring data set.</span></span> <span data-ttu-id="da6fe-139">As previsões são armazenadas no repositório de resultados, um novo conjunto de dados no repositório de dados do Databricks.</span><span class="sxs-lookup"><span data-stu-id="da6fe-139">The predictions are stored in the results store, a new data set on the Databricks data store.</span></span>

<span data-ttu-id="da6fe-140">**Agendador**.</span><span class="sxs-lookup"><span data-stu-id="da6fe-140">**Scheduler**.</span></span> <span data-ttu-id="da6fe-141">Um trabalho do Databricks [agendado][job] lida com pontuação em lote com o modelo Spark.</span><span class="sxs-lookup"><span data-stu-id="da6fe-141">A scheduled Databricks [job][job] handles batch scoring with the Spark model.</span></span> <span data-ttu-id="da6fe-142">O trabalho executa o notebook de pipeline de pontuação, passando argumentos variáveis por meio de parâmetros de bloco de anotações para especificar os detalhes para a construção de conjunto de dados de pontuação e onde armazenar o conjunto de dados de resultados.</span><span class="sxs-lookup"><span data-stu-id="da6fe-142">The job executes the scoring pipeline notebook, passing variable arguments through notebook parameters to specify the details for constructing the scoring data set and where to store the results data set.</span></span>

<span data-ttu-id="da6fe-143">O cenário é criado como um fluxo de pipeline.</span><span class="sxs-lookup"><span data-stu-id="da6fe-143">The scenario is constructed as a pipeline flow.</span></span> <span data-ttu-id="da6fe-144">Cada notebook é otimizado para executar em uma configuração de lote para cada uma das operações: ingestão, engenharia de recursos, criação de modelo e pontuações do modelo.</span><span class="sxs-lookup"><span data-stu-id="da6fe-144">Each notebook is optimized to perform in a batch setting for each of the operations: ingestion, feature engineering, model building, and model scorings.</span></span> <span data-ttu-id="da6fe-145">Para fazer isso, o notebook de engenharia de recursos foi projetado para gerar um conjunto de dados geral para qualquer operação de treinamento, calibração teste ou pontuação.</span><span class="sxs-lookup"><span data-stu-id="da6fe-145">To accomplish this, the feature engineering notebook is designed to generate a general data set for any of the training, calibration, testing, or scoring operations.</span></span> <span data-ttu-id="da6fe-146">Nesse cenário, usamos uma estratégia de divisão temporal para essas operações, portanto, os parâmetros de notebooks são usados para definir a filtragem do intervalo de datas.</span><span class="sxs-lookup"><span data-stu-id="da6fe-146">In this scenario, we use a temporal split strategy for these operations, so the notebook parameters are used to set date-range filtering.</span></span>

<span data-ttu-id="da6fe-147">Como o cenário cria um pipeline de lote, fornecemos um conjunto de notebooks de exame opcionais para explorar a saída dos notebooks de pipeline.</span><span class="sxs-lookup"><span data-stu-id="da6fe-147">Because the scenario creates a batch pipeline, we provide a set of optional examination notebooks to explore the output of the pipeline notebooks.</span></span> <span data-ttu-id="da6fe-148">Você pode encontrar esses no repositório do GitHub:</span><span class="sxs-lookup"><span data-stu-id="da6fe-148">You can find these in the GitHub repository:</span></span>

- `1a_raw-data_exploring`
- `2a_feature_exploration`
- `2b_model_testing`
- `3b_model_scoring_evaluation`

## <a name="recommendations"></a><span data-ttu-id="da6fe-149">Recomendações</span><span class="sxs-lookup"><span data-stu-id="da6fe-149">Recommendations</span></span>

<span data-ttu-id="da6fe-150">O Databricks está configurado para que você possa carregar e implantar seus modelos treinados para fazer previsões com novos dados.</span><span class="sxs-lookup"><span data-stu-id="da6fe-150">Databricks is set up so you can load and deploy your trained models to make predictions with new data.</span></span> <span data-ttu-id="da6fe-151">Usamos o Databricks para esse cenário porque fornece essas vantagens adicionais:</span><span class="sxs-lookup"><span data-stu-id="da6fe-151">We used Databricks for this scenario because it provides these additional advantages:</span></span>

- <span data-ttu-id="da6fe-152">Suporte a logon único usando as credenciais do Azure Active Directory.</span><span class="sxs-lookup"><span data-stu-id="da6fe-152">Single sign-on support using Azure Active Directory credentials.</span></span>
- <span data-ttu-id="da6fe-153">Agendador de trabalhos para executar os trabalhos para os pipelines de produção.</span><span class="sxs-lookup"><span data-stu-id="da6fe-153">Job scheduler to execute jobs for production pipelines.</span></span>
- <span data-ttu-id="da6fe-154">O notebook completamente interativo com colaboração, painéis e APIS de REST.</span><span class="sxs-lookup"><span data-stu-id="da6fe-154">Fully interactive notebook with collaboration, dashboards, REST APIs.</span></span>
- <span data-ttu-id="da6fe-155">Clusters ilimitados que podem ser dimensionados para qualquer tamanho.</span><span class="sxs-lookup"><span data-stu-id="da6fe-155">Unlimited clusters that can scale to any size.</span></span>
- <span data-ttu-id="da6fe-156">Segurança avançada, controles de acesso baseado em função e os logs de auditoria.</span><span class="sxs-lookup"><span data-stu-id="da6fe-156">Advanced security, role-based access controls, and audit logs.</span></span>

<span data-ttu-id="da6fe-157">Para interagir com o serviço do Azure Databricks, use o Databricks [Espaço de trabalho][workspace] interface em um navegador da web ou a [interface de linha de comando][cli] (CLI).</span><span class="sxs-lookup"><span data-stu-id="da6fe-157">To interact with the Azure Databricks service, use the Databricks [Workspace][workspace] interface in a web browser or the [command-line interface][cli] (CLI).</span></span> <span data-ttu-id="da6fe-158">Acesse a CLI do Databricks de qualquer plataforma que dá suporte ao Python 2.7.9 a 3.6.</span><span class="sxs-lookup"><span data-stu-id="da6fe-158">Access the Databricks CLI from any platform that supports Python 2.7.9 to 3.6.</span></span>

<span data-ttu-id="da6fe-159">A implementação de referência usa os [notebooks][notebooks] para executar tarefas em sequência.</span><span class="sxs-lookup"><span data-stu-id="da6fe-159">The reference implementation uses [notebooks][notebooks] to execute tasks in sequence.</span></span> <span data-ttu-id="da6fe-160">Cada notebook armazena artefatos de dados intermediários (treinamento, teste, de pontuação ou conjuntos de resultados de dados) para o mesmo armazenamento de dados como dados de entrada.</span><span class="sxs-lookup"><span data-stu-id="da6fe-160">Each notebook stores intermediate data artifacts (training, test, scoring, or results data sets) to the same data store as the input data.</span></span> <span data-ttu-id="da6fe-161">O objetivo é tornar mais fácil para que você possa usá-lo conforme necessário no seu caso de uso específico.</span><span class="sxs-lookup"><span data-stu-id="da6fe-161">The goal is to make it easy for you to use it as needed in your particular use case.</span></span> <span data-ttu-id="da6fe-162">Na prática, se conectar a fonte de dados à instância do Azure Databricks aos notebooks para ler e gravar diretamente de volta no seu armazenamento.</span><span class="sxs-lookup"><span data-stu-id="da6fe-162">In practice, you would connect your data source to your Azure Databricks instance for the notebooks to read and write directly back into your storage.</span></span>

<span data-ttu-id="da6fe-163">Você pode monitorar a execução do trabalho por meio da interface do usuário Databricks, o armazenamento de dados ou a [CLI][cli] do Databricks conforme necessário.</span><span class="sxs-lookup"><span data-stu-id="da6fe-163">You can monitor job execution through the Databricks user interface, the data store, or the Databricks [CLI][cli] as necessary.</span></span> <span data-ttu-id="da6fe-164">Monitorar o cluster usando o [log de eventos][log] e outras [métricas][ metrics] que o Databricks fornece.</span><span class="sxs-lookup"><span data-stu-id="da6fe-164">Monitor the cluster using the [event log][log] and other [metrics][metrics] that Databricks provides.</span></span>

## <a name="performance-considerations"></a><span data-ttu-id="da6fe-165">Considerações sobre o desempenho</span><span class="sxs-lookup"><span data-stu-id="da6fe-165">Performance considerations</span></span>

<span data-ttu-id="da6fe-166">Um cluster do Azure Databricks habilita o dimensionamento automático por padrão, de modo que, durante o tempo de execução, o Databricks realoca dinamicamente os trabalhadores para levar em conta as características do seu trabalho.</span><span class="sxs-lookup"><span data-stu-id="da6fe-166">An Azure Databricks cluster enables autoscaling by default so that during runtime, Databricks dynamically reallocates workers to account for the characteristics of your job.</span></span> <span data-ttu-id="da6fe-167">Certas partes do seu pipeline podem ser mais exigentes que outras.</span><span class="sxs-lookup"><span data-stu-id="da6fe-167">Certain parts of your pipeline may be more computationally demanding than others.</span></span> <span data-ttu-id="da6fe-168">O Databricks adiciona trabalhadores adicionais durante essas fases do seu trabalho (e remove-os quando não forem mais necessários).</span><span class="sxs-lookup"><span data-stu-id="da6fe-168">Databricks adds additional workers during these phases of your job (and removes them when they’re no longer needed).</span></span> <span data-ttu-id="da6fe-169">Dimensionamento automático torna mais fácil de obter alta [utilização do cluster][cluster], porque você não precisa provisionar o cluster de acordo com uma carga de trabalho.</span><span class="sxs-lookup"><span data-stu-id="da6fe-169">Autoscaling makes it easier to achieve high [cluster utilization][cluster], because you don’t need to provision the cluster to match a workload.</span></span>

<span data-ttu-id="da6fe-170">Além disso, os pipelines agendados mais complexos podem ser desenvolvidos usando o [Azure Data Factory][adf] com o Azure Databricks.</span><span class="sxs-lookup"><span data-stu-id="da6fe-170">Additionally, more complex scheduled pipelines can be developed by using [Azure Data Factory][adf] with Azure Databricks.</span></span>

## <a name="storage-considerations"></a><span data-ttu-id="da6fe-171">Considerações de armazenamento</span><span class="sxs-lookup"><span data-stu-id="da6fe-171">Storage considerations</span></span>

<span data-ttu-id="da6fe-172">Nessas implementações de referência, os dados são armazenados diretamente no armazenamento do Databricks para manter a simplicidade.</span><span class="sxs-lookup"><span data-stu-id="da6fe-172">In this reference implementation, the data is stored directly within Databricks storage for simplicity.</span></span> <span data-ttu-id="da6fe-173">Em um ambiente de produção, no entanto, os dados podem ser armazenados no armazenamento de dados de nuvem, como [Armazenamento de Blob do Azure][blob].</span><span class="sxs-lookup"><span data-stu-id="da6fe-173">In a production setting, however, the data can be stored on cloud data storage such as [Azure Blob Storage][blob].</span></span> <span data-ttu-id="da6fe-174">[Databricks][databricks-connect] também dá suporte ao Azure Data Lake Store, Azure SQL Data Warehouse, Azure Cosmos DB, Apache Kafka e Hadoop.</span><span class="sxs-lookup"><span data-stu-id="da6fe-174">[Databricks][databricks-connect] also supports Azure Data Lake Store, Azure SQL Data Warehouse, Azure Cosmos DB, Apache Kafka, and Hadoop.</span></span>

## <a name="cost-considerations"></a><span data-ttu-id="da6fe-175">Considerações de custo</span><span class="sxs-lookup"><span data-stu-id="da6fe-175">Cost considerations</span></span>

<span data-ttu-id="da6fe-176">O Azure Databricks é um Spark premium oferecendo com um custo associado.</span><span class="sxs-lookup"><span data-stu-id="da6fe-176">Azure Databricks is a premium Spark offering with an associated cost.</span></span> <span data-ttu-id="da6fe-177">Além disso, [tipos de preço][pricing] de Databricks standard e premium.</span><span class="sxs-lookup"><span data-stu-id="da6fe-177">In addition, there are standard and premium Databricks [pricing tiers][pricing].</span></span>

<span data-ttu-id="da6fe-178">Para este cenário, o tipo de preço standard é suficiente.</span><span class="sxs-lookup"><span data-stu-id="da6fe-178">For this scenario, the standard pricing tier is sufficient.</span></span> <span data-ttu-id="da6fe-179">No entanto, se seu aplicativo específico requer que o dimensionamento automático de clusters lide com cargas de trabalho maiores ou painéis interativos do Databricks, o nível premium pode aumentar ainda mais os custos.</span><span class="sxs-lookup"><span data-stu-id="da6fe-179">However, if your specific application requires automatically scaling clusters to handle larger workloads or interactive Databricks dashboards, the premium level could increase costs further.</span></span>

<span data-ttu-id="da6fe-180">Os notebooks de solução podem ser executados em qualquer plataforma com base no Spark com edições mínimas para remover os pacotes específicos do Databricks.</span><span class="sxs-lookup"><span data-stu-id="da6fe-180">The solution notebooks can run on any Spark-based platform with minimal edits to remove the Databricks-specific packages.</span></span> <span data-ttu-id="da6fe-181">Consulte as seguintes soluções semelhantes para várias plataformas do Azure:</span><span class="sxs-lookup"><span data-stu-id="da6fe-181">See the following similar solutions for various Azure platforms:</span></span>

- <span data-ttu-id="da6fe-182">[Python no Azure Machine Learning Studio][python-aml]</span><span class="sxs-lookup"><span data-stu-id="da6fe-182">[Python on Azure Machine Learning Studio][python-aml]</span></span>
- <span data-ttu-id="da6fe-183">[Serviços de R do SQL Server][sql-r]</span><span class="sxs-lookup"><span data-stu-id="da6fe-183">[SQL Server R services][sql-r]</span></span>
- <span data-ttu-id="da6fe-184">[PySpark em uma Máquina Virtual de Ciência de Dados do Azure][py-dvsm]</span><span class="sxs-lookup"><span data-stu-id="da6fe-184">[PySpark on an Azure Data Science Virtual Machine][py-dvsm]</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="da6fe-185">Implantar a solução</span><span class="sxs-lookup"><span data-stu-id="da6fe-185">Deploy the solution</span></span>

<span data-ttu-id="da6fe-186">Para implantar essa arquitetura de referência, siga as etapas descritas no repositório  [GitHub][github] para criar uma solução escalonável para modelos de pontuação Spark em lote no Azure Databricks.</span><span class="sxs-lookup"><span data-stu-id="da6fe-186">To deploy this reference architecture, follow the steps described in the [GitHub][github] repository to build a scalable solution for scoring Spark models in batch on Azure Databricks.</span></span>

## <a name="related-architectures"></a><span data-ttu-id="da6fe-187">Arquiteturas relacionadas</span><span class="sxs-lookup"><span data-stu-id="da6fe-187">Related architectures</span></span>

<span data-ttu-id="da6fe-188">Também criamos uma arquitetura de referência que usa o Spark para construir [sistemas de recomendação em tempo real] [ recommendation] com pontuações offline, pré-computadas.</span><span class="sxs-lookup"><span data-stu-id="da6fe-188">We have also built a reference architecture that uses Spark for building [real-time recommendation systems][recommendation] with offline, pre-computed scores.</span></span> <span data-ttu-id="da6fe-189">Esses sistemas de recomendação são cenários comuns em que as pontuações são processadas em lote.</span><span class="sxs-lookup"><span data-stu-id="da6fe-189">These recommendation systems are common scenarios where scores are batch-processed.</span></span>

[adf]: https://azure.microsoft.com/blog/operationalize-azure-databricks-notebooks-using-data-factory/
[ai-guide]: /azure/machine-learning/team-data-science-process/cortana-analytics-playbook-predictive-maintenance
[blob]: https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html
[cli]: https://docs.databricks.com/user-guide/dev-tools/databricks-cli.html
[cluster]: https://docs.azuredatabricks.net/user-guide/clusters/sizing.html
[databricks]: /azure/azure-databricks/
[databricks-connect]: /azure/azure-databricks/databricks-connect-to-data-sources
[data-sources]: https://docs.databricks.com/spark/latest/data-sources/index.html
[github]: https://github.com/Azure/BatchSparkScoringPredictiveMaintenance
[job]: https://docs.databricks.com/user-guide/jobs.html
[log]: https://docs.databricks.com/user-guide/clusters/event-log.html
[metrics]: https://docs.databricks.com/user-guide/clusters/metrics.html
[mllib]: https://docs.databricks.com/spark/latest/mllib/index.html
[mllib-spark]: https://docs.databricks.com/spark/latest/mllib/index.html#apache-spark-mllib
[notebooks]: https://docs.databricks.com/user-guide/notebooks/index.html
[pricing]: https://azure.microsoft.com/en-us/pricing/details/databricks/
[python-aml]: https://gallery.azure.ai/Notebook/Predictive-Maintenance-Modelling-Guide-Python-Notebook-1
[py-dvsm]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-using-PySpark
[recommendation]: /azure/architecture/reference-architectures/ai/real-time-recommendation
[sql-r]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-Modeling-Guide-using-SQL-R-Services-1
[workspace]: https://docs.databricks.com/user-guide/workspace.html
