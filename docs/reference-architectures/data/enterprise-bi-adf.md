---
title: Business intelligence (BI) corporativo automatizado
titleSuffix: Azure Reference Architectures
description: Automatize um fluxo de trabalho de extração, carga e transformação (ELT) no Azure usando o Azure Data Factory com o SQL Data Warehouse.
author: MikeWasson
ms.date: 11/06/2018
ms.topic: reference-architecture
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: seodec18
ms.openlocfilehash: 020c401e9db85b76fd48c6df9be9c80d2ba5c7e4
ms.sourcegitcommit: 1b50810208354577b00e89e5c031b774b02736e2
ms.translationtype: HT
ms.contentlocale: pt-BR
ms.lasthandoff: 01/23/2019
ms.locfileid: "54481466"
---
# <a name="automated-enterprise-bi-with-sql-data-warehouse-and-azure-data-factory"></a><span data-ttu-id="48301-103">Enterprise BI automatizada com o SQL Data Warehouse e Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="48301-103">Automated enterprise BI with SQL Data Warehouse and Azure Data Factory</span></span>

<span data-ttu-id="48301-104">Essa arquitetura de referência mostra como executar um carregamento incremental em um pipeline [ELT (extrair-carregar-transformar)](../../data-guide/relational-data/etl.md#extract-load-and-transform-elt).</span><span class="sxs-lookup"><span data-stu-id="48301-104">This reference architecture shows how to perform incremental loading in an [extract, load, and transform (ELT)](../../data-guide/relational-data/etl.md#extract-load-and-transform-elt) pipeline.</span></span> <span data-ttu-id="48301-105">Ela usa o Azure Data Factory para automatizar o pipeline ELT.</span><span class="sxs-lookup"><span data-stu-id="48301-105">It uses Azure Data Factory to automate the ELT pipeline.</span></span> <span data-ttu-id="48301-106">O pipeline move incrementalmente os dados de OLTP mais recentes de um banco de dados do SQL Server local no SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="48301-106">The pipeline incrementally moves the latest OLTP data from an on-premises SQL Server database into SQL Data Warehouse.</span></span> <span data-ttu-id="48301-107">Os dados transacionais são transformados em um modelo de tabela para análise.</span><span class="sxs-lookup"><span data-stu-id="48301-107">Transactional data is transformed into a tabular model for analysis.</span></span>

> [!VIDEO https://www.microsoft.com/en-us/videoplayer/embed/RE2Gnz2]

<span data-ttu-id="48301-108">Há uma implantação de referência para essa arquitetura de referência disponível no [GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="48301-108">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Diagrama de arquitetura para BI corporativo automatizado com o SQL Data Warehouse e Azure Data Factory](./images/enterprise-bi-sqldw-adf.png)

<span data-ttu-id="48301-110">Essa arquitetura é compilada naquela mostrada em [Enterprise BI com o SQL Data Warehouse](./enterprise-bi-sqldw.md), mas também adiciona alguns recursos que são importantes para cenários de data warehouse de dados da empresa.</span><span class="sxs-lookup"><span data-stu-id="48301-110">This architecture builds on the one shown in [Enterprise BI with SQL Data Warehouse](./enterprise-bi-sqldw.md), but adds some features that are important for enterprise data warehousing scenarios.</span></span>

- <span data-ttu-id="48301-111">Automação do pipeline usando o Data Factory.</span><span class="sxs-lookup"><span data-stu-id="48301-111">Automation of the pipeline using Data Factory.</span></span>
- <span data-ttu-id="48301-112">Carregamento incremental.</span><span class="sxs-lookup"><span data-stu-id="48301-112">Incremental loading.</span></span>
- <span data-ttu-id="48301-113">Integrando várias fontes de dados.</span><span class="sxs-lookup"><span data-stu-id="48301-113">Integrating multiple data sources.</span></span>
- <span data-ttu-id="48301-114">Carregando dados binários como imagens e dados geoespaciais.</span><span class="sxs-lookup"><span data-stu-id="48301-114">Loading binary data such as geospatial data and images.</span></span>

## <a name="architecture"></a><span data-ttu-id="48301-115">Arquitetura</span><span class="sxs-lookup"><span data-stu-id="48301-115">Architecture</span></span>

<span data-ttu-id="48301-116">A arquitetura consiste nos componentes a seguir.</span><span class="sxs-lookup"><span data-stu-id="48301-116">The architecture consists of the following components.</span></span>

### <a name="data-sources"></a><span data-ttu-id="48301-117">Fontes de dados</span><span class="sxs-lookup"><span data-stu-id="48301-117">Data sources</span></span>

<span data-ttu-id="48301-118">**SQL Server local**.</span><span class="sxs-lookup"><span data-stu-id="48301-118">**On-premises SQL Server**.</span></span> <span data-ttu-id="48301-119">A fonte de dados está localizada em um banco de dados SQL Server local.</span><span class="sxs-lookup"><span data-stu-id="48301-119">The source data is located in a SQL Server database on premises.</span></span> <span data-ttu-id="48301-120">Para simular o ambiente local, os scripts de implantação dessa arquitetura provisionam uma máquina virtual no Azure com o SQL Server instalado.</span><span class="sxs-lookup"><span data-stu-id="48301-120">To simulate the on-premises environment, the deployment scripts for this architecture provision a virtual machine in Azure with SQL Server installed.</span></span> <span data-ttu-id="48301-121">O [banco de dados de exemplo de OLTP de World Wide Importers][wwi] é usado como o banco de dados de origem.</span><span class="sxs-lookup"><span data-stu-id="48301-121">The [Wide World Importers OLTP sample database][wwi] is used as the source database.</span></span>

<span data-ttu-id="48301-122">**Dados externos**.</span><span class="sxs-lookup"><span data-stu-id="48301-122">**External data**.</span></span> <span data-ttu-id="48301-123">Um cenário comum para data warehouses é integrar várias fontes de dados.</span><span class="sxs-lookup"><span data-stu-id="48301-123">A common scenario for data warehouses is to integrate multiple data sources.</span></span> <span data-ttu-id="48301-124">Essa arquitetura de referência carrega um conjunto de dados externos que contém as populações de cidade por ano e integra-se aos dados do banco de dados OLTP.</span><span class="sxs-lookup"><span data-stu-id="48301-124">This reference architecture loads an external data set that contains city populations by year, and integrates it with the data from the OLTP database.</span></span> <span data-ttu-id="48301-125">Você pode usar esses dados para insights, como: "O crescimento das vendas em cada região corresponde ou excede o crescimento demográfico?"</span><span class="sxs-lookup"><span data-stu-id="48301-125">You can use this data for insights such as: "Does sales growth in each region match or exceed population growth?"</span></span>

### <a name="ingestion-and-data-storage"></a><span data-ttu-id="48301-126">Ingestão e armazenamento de dados</span><span class="sxs-lookup"><span data-stu-id="48301-126">Ingestion and data storage</span></span>

<span data-ttu-id="48301-127">**Armazenamento de Blobs**.</span><span class="sxs-lookup"><span data-stu-id="48301-127">**Blob Storage**.</span></span> <span data-ttu-id="48301-128">O armazenamento de blobs é usado como uma área de preparo dos dados de origem antes de carregá-los no SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="48301-128">Blob storage is used as a staging area for the source data before loading it into SQL Data Warehouse.</span></span>

<span data-ttu-id="48301-129">**SQL Data Warehouse do Azure**.</span><span class="sxs-lookup"><span data-stu-id="48301-129">**Azure SQL Data Warehouse**.</span></span> <span data-ttu-id="48301-130">O [SQL Data Warehouse](/azure/sql-data-warehouse/) é um sistema distribuído projetado para executar uma análise em grandes quantidades de dados.</span><span class="sxs-lookup"><span data-stu-id="48301-130">[SQL Data Warehouse](/azure/sql-data-warehouse/) is a distributed system designed to perform analytics on large data.</span></span> <span data-ttu-id="48301-131">Ele dá suporte a MPP (processamento altamente paralelo), que o torna adequado para executar análise de alto desempenho.</span><span class="sxs-lookup"><span data-stu-id="48301-131">It supports massive parallel processing (MPP), which makes it suitable for running high-performance analytics.</span></span>

<span data-ttu-id="48301-132">**Azure Data Factory**.</span><span class="sxs-lookup"><span data-stu-id="48301-132">**Azure Data Factory**.</span></span> <span data-ttu-id="48301-133">O [Data Factory][adf] é um serviço gerenciado que orquestra e automatiza a movimentação e a transformação de dados.</span><span class="sxs-lookup"><span data-stu-id="48301-133">[Data Factory][adf] is a managed service that orchestrates and automates data movement and data transformation.</span></span> <span data-ttu-id="48301-134">Nessa arquitetura, ele coordena os diversos estágios do processo de ELT.</span><span class="sxs-lookup"><span data-stu-id="48301-134">In this architecture, it coordinates the various stages of the ELT process.</span></span>

### <a name="analysis-and-reporting"></a><span data-ttu-id="48301-135">Análise e relatórios</span><span class="sxs-lookup"><span data-stu-id="48301-135">Analysis and reporting</span></span>

<span data-ttu-id="48301-136">**Azure Analysis Services**.</span><span class="sxs-lookup"><span data-stu-id="48301-136">**Azure Analysis Services**.</span></span> <span data-ttu-id="48301-137">O [Analysis Services](/azure/analysis-services/) é um serviço totalmente gerenciado que fornece recursos de modelagem de dados.</span><span class="sxs-lookup"><span data-stu-id="48301-137">[Analysis Services](/azure/analysis-services/) is a fully managed service that provides data modeling capabilities.</span></span> <span data-ttu-id="48301-138">O modelo semântico é carregado no Analysis Services.</span><span class="sxs-lookup"><span data-stu-id="48301-138">The semantic model is loaded into Analysis Services.</span></span>

<span data-ttu-id="48301-139">**Power BI**.</span><span class="sxs-lookup"><span data-stu-id="48301-139">**Power BI**.</span></span> <span data-ttu-id="48301-140">O Power BI é um conjunto de ferramentas de análise de negócios para analisar dados a fim de obter informações comerciais.</span><span class="sxs-lookup"><span data-stu-id="48301-140">Power BI is a suite of business analytics tools to analyze data for business insights.</span></span> <span data-ttu-id="48301-141">Nessa arquitetura, ele consulta o modelo semântico armazenado no Analysis Services.</span><span class="sxs-lookup"><span data-stu-id="48301-141">In this architecture, it queries the semantic model stored in Analysis Services.</span></span>

### <a name="authentication"></a><span data-ttu-id="48301-142">Autenticação</span><span class="sxs-lookup"><span data-stu-id="48301-142">Authentication</span></span>

<span data-ttu-id="48301-143">O **Azure AD** (Azure Active Directory) autentica os usuários que se conectam ao servidor do Analysis Services por meio do Power BI.</span><span class="sxs-lookup"><span data-stu-id="48301-143">**Azure Active Directory** (Azure AD) authenticates users who connect to the Analysis Services server through Power BI.</span></span>

<span data-ttu-id="48301-144">O Data Factory também pode usar o Azure AD para autenticar no SQL Data Warehouse por meio de uma entidade de serviço ou uma Identidade de Serviço Gerenciada (MSI).</span><span class="sxs-lookup"><span data-stu-id="48301-144">Data Factory can use also use Azure AD to authenticate to SQL Data Warehouse, by using a service principal or Managed Service Identity (MSI).</span></span> <span data-ttu-id="48301-145">Para manter a simplicidade, a implantação de exemplo usa a autenticação do SQL Server.</span><span class="sxs-lookup"><span data-stu-id="48301-145">For simplicity, the example deployment uses SQL Server authentication.</span></span>

## <a name="data-pipeline"></a><span data-ttu-id="48301-146">Pipeline de dados</span><span class="sxs-lookup"><span data-stu-id="48301-146">Data pipeline</span></span>

<span data-ttu-id="48301-147">No [Azure Data Factory][adf], um pipeline é um agrupamento lógico de atividades usadas para coordenar uma tarefa &mdash; nesse caso, carregando e transformando dados no SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="48301-147">In [Azure Data Factory][adf], a pipeline is a logical grouping of activities used to coordinate a task &mdash; in this case, loading and transforming data into SQL Data Warehouse.</span></span>

<span data-ttu-id="48301-148">Essa arquitetura de referência define um pipeline mestre que executa uma sequência de pipelines filho.</span><span class="sxs-lookup"><span data-stu-id="48301-148">This reference architecture defines a master pipeline that runs a sequence of child pipelines.</span></span> <span data-ttu-id="48301-149">Cada pipeline filho carrega dados em uma ou mais tabelas do data warehouse.</span><span class="sxs-lookup"><span data-stu-id="48301-149">Each child pipeline loads data into one or more data warehouse tables.</span></span>

![Captura de tela do pipeline no Azure Data Factory](./images/adf-pipeline.png)

## <a name="incremental-loading"></a><span data-ttu-id="48301-151">Carregamento incremental</span><span class="sxs-lookup"><span data-stu-id="48301-151">Incremental loading</span></span>

<span data-ttu-id="48301-152">Quando você executa um processo automatizado de ETL ou ELT, é mais eficiente carregar apenas os dados alterados desde a execução anterior.</span><span class="sxs-lookup"><span data-stu-id="48301-152">When you run an automated ETL or ELT process, it's most efficient to load only the data that changed since the previous run.</span></span> <span data-ttu-id="48301-153">Isso é chamado de um *carregamento incremental* em vez de um carregamento completo que carrega todos os dados.</span><span class="sxs-lookup"><span data-stu-id="48301-153">This is called an *incremental load*, as opposed to a full load that loads all of the data.</span></span> <span data-ttu-id="48301-154">Para executar um carregamento incremental, você precisa de uma maneira de identificar quais dados foram alterados.</span><span class="sxs-lookup"><span data-stu-id="48301-154">To perform an incremental load, you need a way to identify which data has changed.</span></span> <span data-ttu-id="48301-155">A abordagem mais comum é usar um valor *alto de marca d'água*, o que significa acompanhar o valor mais recente de alguma coluna na tabela de origem, uma coluna de data e hora ou uma coluna de inteiro exclusivo.</span><span class="sxs-lookup"><span data-stu-id="48301-155">The most common approach is to use a *high water mark* value, which means tracking the latest value of some column in the source table, either a datetime column or a unique integer column.</span></span>

<span data-ttu-id="48301-156">Começando com o SQL Server 2016, você pode usar [tabelas temporais](/sql/relational-databases/tables/temporal-tables).</span><span class="sxs-lookup"><span data-stu-id="48301-156">Starting with SQL Server 2016, you can use [temporal tables](/sql/relational-databases/tables/temporal-tables).</span></span> <span data-ttu-id="48301-157">Tratam-se de tabelas com versão do sistema que mantêm um histórico completo das alterações de dados.</span><span class="sxs-lookup"><span data-stu-id="48301-157">These are system-versioned tables that keep a full history of data changes.</span></span> <span data-ttu-id="48301-158">O mecanismo de banco de dados registra automaticamente o histórico de todas as alterações em uma tabela de histórico separada.</span><span class="sxs-lookup"><span data-stu-id="48301-158">The database engine automatically records the history of every change in a separate history table.</span></span> <span data-ttu-id="48301-159">Você pode consultar os dados históricos com a adição de uma cláusula FOR SYSTEM_TIME para uma consulta.</span><span class="sxs-lookup"><span data-stu-id="48301-159">You can query the historical data by adding a FOR SYSTEM_TIME clause to a query.</span></span> <span data-ttu-id="48301-160">Internamente, o mecanismo de banco de dados consulta a tabela do histórico, mas isso é transparente ao aplicativo.</span><span class="sxs-lookup"><span data-stu-id="48301-160">Internally, the database engine queries the history table, but this is transparent to the application.</span></span>

> [!NOTE]
> <span data-ttu-id="48301-161">Para versões anteriores do SQL Server, você pode usar a CDC ([Captura de Dados de Alterações](/sql/relational-databases/track-changes/about-change-data-capture-sql-server)).</span><span class="sxs-lookup"><span data-stu-id="48301-161">For earlier versions of SQL Server, you can use [Change Data Capture](/sql/relational-databases/track-changes/about-change-data-capture-sql-server) (CDC).</span></span> <span data-ttu-id="48301-162">Essa abordagem é menos conveniente do que as tabelas temporais porque você tem de consultar uma tabela separada, e as alterações são controladas por um número de sequência de log em vez de um carimbo de data/hora.</span><span class="sxs-lookup"><span data-stu-id="48301-162">This approach is less convenient than temporal tables, because you have to query a separate change table, and changes are tracked by a log sequence number, rather than a timestamp.</span></span>
>

<span data-ttu-id="48301-163">Tabelas temporais são úteis para dados de dimensão, os quais podem ser alterados ao longo do tempo.</span><span class="sxs-lookup"><span data-stu-id="48301-163">Temporal tables are useful for dimension data, which can change over time.</span></span> <span data-ttu-id="48301-164">Tabelas de fatos geralmente representam uma transação imutável, como uma venda, caso em que não faz sentido manter o histórico de versão do sistema.</span><span class="sxs-lookup"><span data-stu-id="48301-164">Fact tables usually represent an immutable transaction such as a sale, in which case keeping the system version history doesn't make sense.</span></span> <span data-ttu-id="48301-165">Em vez disso, as transações normalmente contam com uma coluna que representa a data da transação, que pode ser usada como o valor de marca d'água.</span><span class="sxs-lookup"><span data-stu-id="48301-165">Instead, transactions usually have a column that represents the transaction date, which can be used as the watermark value.</span></span> <span data-ttu-id="48301-166">Por exemplo, no banco de dados OLTP da Wide World Importers, as tabelas Sales.Invoices e Sales.InvoiceLines têm um campo `LastEditedWhen` que é padrão para `sysdatetime()`.</span><span class="sxs-lookup"><span data-stu-id="48301-166">For example, in the Wide World Importers OLTP database, the Sales.Invoices and Sales.InvoiceLines tables have a `LastEditedWhen` field that defaults to `sysdatetime()`.</span></span>

<span data-ttu-id="48301-167">Aqui está o fluxo geral para o pipeline ELT:</span><span class="sxs-lookup"><span data-stu-id="48301-167">Here is the general flow for the ELT pipeline:</span></span>

1. <span data-ttu-id="48301-168">Para cada tabela no banco de dados de origem, acompanhe o tempo de corte quando o último trabalho ELT foi executado.</span><span class="sxs-lookup"><span data-stu-id="48301-168">For each table in the source database, track the cutoff time when the last ELT job ran.</span></span> <span data-ttu-id="48301-169">Armazene essas informações no data warehouse.</span><span class="sxs-lookup"><span data-stu-id="48301-169">Store this information in the data warehouse.</span></span> <span data-ttu-id="48301-170">(Durante a instalação inicial, todas as horas são definidas como '1-1-1900'.)</span><span class="sxs-lookup"><span data-stu-id="48301-170">(On initial setup, all times are set to '1-1-1900'.)</span></span>

2. <span data-ttu-id="48301-171">Durante a etapa de exportação de dados, a hora de corte é passada como um parâmetro para um conjunto de procedimentos armazenados no banco de dados de origem.</span><span class="sxs-lookup"><span data-stu-id="48301-171">During the data export step, the cutoff time is passed as a parameter to a set of stored procedures in the source database.</span></span> <span data-ttu-id="48301-172">Esses procedimentos armazenados consultam todos os registros que foram alterados ou criados após a hora de corte.</span><span class="sxs-lookup"><span data-stu-id="48301-172">These stored procedures query for any records that were changed or created after the cutoff time.</span></span> <span data-ttu-id="48301-173">Para a tabela de fatos de Vendas, é usada a coluna `LastEditedWhen`.</span><span class="sxs-lookup"><span data-stu-id="48301-173">For the Sales fact table, the `LastEditedWhen` column is used.</span></span> <span data-ttu-id="48301-174">Para os dados de dimensão, são usadas tabelas temporais com versão do sistema.</span><span class="sxs-lookup"><span data-stu-id="48301-174">For the dimension data, system-versioned temporal tables are used.</span></span>

3. <span data-ttu-id="48301-175">Quando a migração de dados for concluída, atualize a tabela que armazena os tempos de corte.</span><span class="sxs-lookup"><span data-stu-id="48301-175">When the data migration is complete, update the table that stores the cutoff times.</span></span>

<span data-ttu-id="48301-176">Também é útil registrar uma *linhagem* para cada execução de ELT.</span><span class="sxs-lookup"><span data-stu-id="48301-176">It's also useful to record a *lineage* for each ELT run.</span></span> <span data-ttu-id="48301-177">Para um determinado registro, a linhagem associa esse registro com a execução de ELT que produziu os dados.</span><span class="sxs-lookup"><span data-stu-id="48301-177">For a given record, the lineage associates that record with the ELT run that produced the data.</span></span> <span data-ttu-id="48301-178">Para cada execução de ETL, um novo registro de linhagem é criado para cada tabela, mostrando os tempos de carregamento iniciais e finais.</span><span class="sxs-lookup"><span data-stu-id="48301-178">For each ETL run, a new lineage record is created for every table, showing the starting and ending load times.</span></span> <span data-ttu-id="48301-179">As chaves de linhagem para cada registro são armazenadas nas tabelas de dimensões e de fatos.</span><span class="sxs-lookup"><span data-stu-id="48301-179">The lineage keys for each record are stored in the dimension and fact tables.</span></span>

![Captura de tela da tabela de dimensões da cidade](./images/city-dimension-table.png)

<span data-ttu-id="48301-181">Depois que um novo lote de dados for carregado no warehouse, atualize o modelo de tabela do Analysis Services.</span><span class="sxs-lookup"><span data-stu-id="48301-181">After a new batch of data is loaded into the warehouse, refresh the Analysis Services tabular model.</span></span> <span data-ttu-id="48301-182">Consulte [Atualização assíncrona com a API REST](/azure/analysis-services/analysis-services-async-refresh).</span><span class="sxs-lookup"><span data-stu-id="48301-182">See [Asynchronous refresh with the REST API](/azure/analysis-services/analysis-services-async-refresh).</span></span>

## <a name="data-cleansing"></a><span data-ttu-id="48301-183">Limpeza de dados</span><span class="sxs-lookup"><span data-stu-id="48301-183">Data cleansing</span></span>

<span data-ttu-id="48301-184">A limpeza de dados deve ser parte do processo de ELT.</span><span class="sxs-lookup"><span data-stu-id="48301-184">Data cleansing should be part of the ELT process.</span></span> <span data-ttu-id="48301-185">Nessa arquitetura de referência, uma fonte de dados incorretos é a tabela de população da cidade, em que algumas cidades apresentam zero população, talvez porque nenhum dado estivesse disponível.</span><span class="sxs-lookup"><span data-stu-id="48301-185">In this reference architecture, one source of bad data is the city population table, where some cities have zero population, perhaps because no data was available.</span></span> <span data-ttu-id="48301-186">Durante o processamento, o pipeline ELT remove essas cidades da tabela de população da cidade.</span><span class="sxs-lookup"><span data-stu-id="48301-186">During processing, the ELT pipeline removes those cities from the city population table.</span></span> <span data-ttu-id="48301-187">Execute a limpeza de dados em tabelas de preparo em vez de tabelas externas.</span><span class="sxs-lookup"><span data-stu-id="48301-187">Perform data cleansing on staging tables, rather than external tables.</span></span>

<span data-ttu-id="48301-188">Aqui está o procedimento armazenado que remove as cidades com zero população da tabela de população da cidade.</span><span class="sxs-lookup"><span data-stu-id="48301-188">Here is the stored procedure that removes the cities with zero population from the City Population table.</span></span> <span data-ttu-id="48301-189">(Você pode encontrar o arquivo de origem [aqui](https://github.com/mspnp/reference-architectures/blob/master/data/enterprise_bi_sqldw_advanced/azure/sqldw_scripts/citypopulation/%5BIntegration%5D.%5BMigrateExternalCityPopulationData%5D.sql).)</span><span class="sxs-lookup"><span data-stu-id="48301-189">(You can find the source file [here](https://github.com/mspnp/reference-architectures/blob/master/data/enterprise_bi_sqldw_advanced/azure/sqldw_scripts/citypopulation/%5BIntegration%5D.%5BMigrateExternalCityPopulationData%5D.sql).)</span></span>

```sql
DELETE FROM [Integration].[CityPopulation_Staging]
WHERE RowNumber in (SELECT DISTINCT RowNumber
FROM [Integration].[CityPopulation_Staging]
WHERE POPULATION = 0
GROUP BY RowNumber
HAVING COUNT(RowNumber) = 4)
```

## <a name="external-data-sources"></a><span data-ttu-id="48301-190">Fontes de dados externas</span><span class="sxs-lookup"><span data-stu-id="48301-190">External data sources</span></span>

<span data-ttu-id="48301-191">Os data warehouses geralmente consolidam dados de várias fontes.</span><span class="sxs-lookup"><span data-stu-id="48301-191">Data warehouses often consolidate data from multiple sources.</span></span> <span data-ttu-id="48301-192">Essa arquitetura de referência carrega uma fonte de dados externa que contém dados demográficos dos dados.</span><span class="sxs-lookup"><span data-stu-id="48301-192">This reference architecture loads an external data source that contains demographics data.</span></span> <span data-ttu-id="48301-193">Esse conjunto de dados está disponível no armazenamento de blobs do Azure como parte do exemplo [WorldWideImportersDW](https://github.com/Microsoft/sql-server-samples/tree/master/samples/databases/wide-world-importers/sample-scripts/polybase).</span><span class="sxs-lookup"><span data-stu-id="48301-193">This dataset is available in Azure blob storage as part of the [WorldWideImportersDW](https://github.com/Microsoft/sql-server-samples/tree/master/samples/databases/wide-world-importers/sample-scripts/polybase) sample.</span></span>

<span data-ttu-id="48301-194">O Azure Data Factory pode copiar diretamente do armazenamento de blob usando o [conector do armazenamento de blobs](/azure/data-factory/connector-azure-blob-storage).</span><span class="sxs-lookup"><span data-stu-id="48301-194">Azure Data Factory can copy directly from blob storage, using the [blob storage connector](/azure/data-factory/connector-azure-blob-storage).</span></span> <span data-ttu-id="48301-195">No entanto, o conector requer uma cadeia de conexão ou uma assinatura de acesso compartilhado para que não possa ser usado para copiar um blob com acesso de leitura público.</span><span class="sxs-lookup"><span data-stu-id="48301-195">However, the connector requires a connection string or a shared access signature, so it can't be used to copy a blob with public read access.</span></span> <span data-ttu-id="48301-196">Como alternativa, você pode usar o PolyBase para criar uma tabela externa no armazenamento de blobs e, em seguida, copiar as tabelas externas no SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="48301-196">As a workaround, you can use PolyBase to create an external table over Blob storage and then copy the external tables into SQL Data Warehouse.</span></span>

## <a name="handling-large-binary-data"></a><span data-ttu-id="48301-197">Manipulação de dados binários grandes</span><span class="sxs-lookup"><span data-stu-id="48301-197">Handling large binary data</span></span>

<span data-ttu-id="48301-198">No banco de dados de origem, a tabela de Cidades tem uma coluna de local que contém um tipo de dados espaciais de [geografia](/sql/t-sql/spatial-geography/spatial-types-geography).</span><span class="sxs-lookup"><span data-stu-id="48301-198">In the source database, the Cities table has a Location column that holds a [geography](/sql/t-sql/spatial-geography/spatial-types-geography) spatial data type.</span></span> <span data-ttu-id="48301-199">O SQL Data Warehouse não oferece suporte ao tipo **geografia** nativamente, portanto, esse campo é convertido em um tipo **varbinary** durante o carregamento.</span><span class="sxs-lookup"><span data-stu-id="48301-199">SQL Data Warehouse doesn't support the **geography** type natively, so this field is converted to a **varbinary** type during loading.</span></span> <span data-ttu-id="48301-200">(Consulte [Soluções alternativas para tipos de dados sem suporte](/azure/sql-data-warehouse/sql-data-warehouse-tables-data-types#unsupported-data-types).)</span><span class="sxs-lookup"><span data-stu-id="48301-200">(See [Workarounds for unsupported data types](/azure/sql-data-warehouse/sql-data-warehouse-tables-data-types#unsupported-data-types).)</span></span>

<span data-ttu-id="48301-201">No entanto, o PolyBase dá suporte a um tamanho máximo de coluna de `varbinary(8000)`, que significa que alguns dados podem ser truncados.</span><span class="sxs-lookup"><span data-stu-id="48301-201">However, PolyBase supports a maximum column size of `varbinary(8000)`, which means some data could be truncated.</span></span> <span data-ttu-id="48301-202">Uma solução alternativa para esse problema é dividir os dados em partes durante a exportação e remontar as partes, da seguinte maneira:</span><span class="sxs-lookup"><span data-stu-id="48301-202">A workaround for this problem is to break the data up into chunks during export, and then reassemble the chunks, as follows:</span></span>

1. <span data-ttu-id="48301-203">Crie uma tabela de preparo temporária para a coluna Local.</span><span class="sxs-lookup"><span data-stu-id="48301-203">Create a temporary staging table for the Location column.</span></span>

2. <span data-ttu-id="48301-204">Para cada cidade, divida os dados de localização em partes de 8.000 bytes, resultando em 1 &ndash; N linhas para cada cidade.</span><span class="sxs-lookup"><span data-stu-id="48301-204">For each city, split the location data into 8000-byte chunks, resulting in 1 &ndash; N rows for each city.</span></span>

3. <span data-ttu-id="48301-205">Para remontar as partes, use o operador T-SQL [PIVOT](/sql/t-sql/queries/from-using-pivot-and-unpivot) para converter linhas em colunas e depois concatenar os valores de coluna para cada cidade.</span><span class="sxs-lookup"><span data-stu-id="48301-205">To reassemble the chunks, use the T-SQL [PIVOT](/sql/t-sql/queries/from-using-pivot-and-unpivot) operator to convert rows into columns and then concatenate the column values for each city.</span></span>

<span data-ttu-id="48301-206">O desafio é que cada cidade será dividida em um número diferente de linhas, dependendo do tamanho dos dados de geografia.</span><span class="sxs-lookup"><span data-stu-id="48301-206">The challenge is that each city will be split into a different number of rows, depending on the size of geography data.</span></span> <span data-ttu-id="48301-207">Para que o operador PIVOT funcione, cada cidade deve ter o mesmo número de linhas.</span><span class="sxs-lookup"><span data-stu-id="48301-207">For the PIVOT operator to work, every city must have the same number of rows.</span></span> <span data-ttu-id="48301-208">Para isso funcionar, a consulta T-SQL (que você pode exibir [aqui][MergeLocation]) realiza alguns truques para preencher as linhas com valores em branco, de modo que cada cidade tenha o mesmo número de colunas após a dinamização.</span><span class="sxs-lookup"><span data-stu-id="48301-208">To make this work, the T-SQL query (which you can view [here][MergeLocation]) does some tricks to pad out the rows with blank values, so that every city has the same number of columns after the pivot.</span></span> <span data-ttu-id="48301-209">A consulta resultante acaba sendo muito mais rápida do que executar o loop pelas linhas uma por vez.</span><span class="sxs-lookup"><span data-stu-id="48301-209">The resulting query turns out to be much faster than looping through the rows one at a time.</span></span>

<span data-ttu-id="48301-210">A mesma abordagem é usada para dados de imagem.</span><span class="sxs-lookup"><span data-stu-id="48301-210">The same approach is used for image data.</span></span>

## <a name="slowly-changing-dimensions"></a><span data-ttu-id="48301-211">Dimensões de alteração lenta</span><span class="sxs-lookup"><span data-stu-id="48301-211">Slowly changing dimensions</span></span>

<span data-ttu-id="48301-212">Dados de dimensão são relativamente estáticos, mas isso pode mudar.</span><span class="sxs-lookup"><span data-stu-id="48301-212">Dimension data is relatively static, but it can change.</span></span> <span data-ttu-id="48301-213">Por exemplo, um produto pode ser reatribuído a uma categoria de produto diferente.</span><span class="sxs-lookup"><span data-stu-id="48301-213">For example, a product might get reassigned to a different product category.</span></span> <span data-ttu-id="48301-214">Há várias abordagens para o tratamento de dimensões de alteração lenta.</span><span class="sxs-lookup"><span data-stu-id="48301-214">There are several approaches to handling slowly changing dimensions.</span></span> <span data-ttu-id="48301-215">Uma técnica comum, chamada [Tipo 2](https://wikipedia.org/wiki/Slowly_changing_dimension#Type_2:_add_new_row), é adicionar um novo registro sempre que uma dimensão for alterada.</span><span class="sxs-lookup"><span data-stu-id="48301-215">A common technique, called [Type 2](https://wikipedia.org/wiki/Slowly_changing_dimension#Type_2:_add_new_row), is to add a new record whenever a dimension changes.</span></span>

<span data-ttu-id="48301-216">Para implementar a abordagem Tipo 2, as tabelas de dimensões precisam de colunas adicionais que especifiquem o intervalo de datas efetivas para um determinado registro.</span><span class="sxs-lookup"><span data-stu-id="48301-216">In order to implement the Type 2 approach, dimension tables need additional columns that specify the effective date range for a given record.</span></span> <span data-ttu-id="48301-217">Além disso, as chaves primárias do banco de dados de origem serão duplicadas, portanto, a tabela de dimensão deve ter uma chave primária artificial.</span><span class="sxs-lookup"><span data-stu-id="48301-217">Also, primary keys from the source database will be duplicated, so the dimension table must have an artificial primary key.</span></span>

<span data-ttu-id="48301-218">A imagem a seguir mostra a tabela Dimension.City.</span><span class="sxs-lookup"><span data-stu-id="48301-218">The following image shows the Dimension.City table.</span></span> <span data-ttu-id="48301-219">A coluna `WWI City ID` é a chave primária do banco de dados de origem.</span><span class="sxs-lookup"><span data-stu-id="48301-219">The `WWI City ID` column is the primary key from the source database.</span></span> <span data-ttu-id="48301-220">A coluna `City Key` é uma chave artificial gerada durante o pipeline ETL.</span><span class="sxs-lookup"><span data-stu-id="48301-220">The `City Key` column is an artificial key generated during the ETL pipeline.</span></span> <span data-ttu-id="48301-221">Além disso, observe que a tabela tem as colunas `Valid From` e `Valid To`, que definem o intervalo de quando cada linha era válida.</span><span class="sxs-lookup"><span data-stu-id="48301-221">Also notice that the table has `Valid From` and `Valid To` columns, which define the range when each row was valid.</span></span> <span data-ttu-id="48301-222">Os valores atuais têm um `Valid To` igual a “9999-12-31”.</span><span class="sxs-lookup"><span data-stu-id="48301-222">Current values have a `Valid To` equal to '9999-12-31'.</span></span>

![Captura de tela da tabela de dimensões da cidade](./images/city-dimension-table.png)

<span data-ttu-id="48301-224">A vantagem dessa abordagem é que ela preserva os dados históricos, que podem ser valiosos para análise.</span><span class="sxs-lookup"><span data-stu-id="48301-224">The advantage of this approach is that it preserves historical data, which can be valuable for analysis.</span></span> <span data-ttu-id="48301-225">No entanto, também significa que haverá várias linhas para a mesma entidade.</span><span class="sxs-lookup"><span data-stu-id="48301-225">However, it also means there will be multiple rows for the same entity.</span></span> <span data-ttu-id="48301-226">Por exemplo, aqui estão os registros que correspondem a `WWI City ID` = 28561:</span><span class="sxs-lookup"><span data-stu-id="48301-226">For example, here are the records that match `WWI City ID` = 28561:</span></span>

![Segunda captura de tela da tabela de dimensões da cidade](./images/city-dimension-table-2.png)

<span data-ttu-id="48301-228">Para cada fato de Vendas, você associa o fato a uma única linha na tabela de dimensões Cidade, correspondente à data da nota fiscal.</span><span class="sxs-lookup"><span data-stu-id="48301-228">For each Sales fact, you want to associate that fact with a single row in City dimension table, corresponding to the invoice date.</span></span> <span data-ttu-id="48301-229">Como parte do processo de ETL, crie uma coluna adicional que</span><span class="sxs-lookup"><span data-stu-id="48301-229">As part of the ETL process, create an additional column that</span></span> 

<span data-ttu-id="48301-230">A consulta de T-SQL a seguir cria uma tabela temporária que associa cada fatura com a chave Cidade correta da tabela de dimensões Cidade.</span><span class="sxs-lookup"><span data-stu-id="48301-230">The following T-SQL query creates a temporary table that associates each invoice with the correct City Key from the City dimension table.</span></span>

```sql
CREATE TABLE CityHolder
WITH (HEAP , DISTRIBUTION = HASH([WWI Invoice ID]))
AS
SELECT DISTINCT s1.[WWI Invoice ID] AS [WWI Invoice ID],
                c.[City Key] AS [City Key]
    FROM [Integration].[Sale_Staging] s1
    CROSS APPLY (
                SELECT TOP 1 [City Key]
                    FROM [Dimension].[City]
                WHERE [WWI City ID] = s1.[WWI City ID]
                    AND s1.[Last Modified When] > [Valid From]
                    AND s1.[Last Modified When] <= [Valid To]
                ORDER BY [Valid From], [City Key] DESC
                ) c

```

<span data-ttu-id="48301-231">Essa tabela é usada para preencher uma coluna na tabela de fatos Vendas:</span><span class="sxs-lookup"><span data-stu-id="48301-231">This table is used to populate a column in the Sales fact table:</span></span>

```sql
UPDATE [Integration].[Sale_Staging]
SET [Integration].[Sale_Staging].[WWI Customer ID] =  CustomerHolder.[WWI Customer ID]
```

<span data-ttu-id="48301-232">Essa coluna permite que uma consulta do Power BI encontre o registro correto de Cidade para uma nota fiscal de vendas específica.</span><span class="sxs-lookup"><span data-stu-id="48301-232">This column enables a Power BI query to find the correct City record for a given sales invoice.</span></span>

## <a name="security-considerations"></a><span data-ttu-id="48301-233">Considerações de segurança</span><span class="sxs-lookup"><span data-stu-id="48301-233">Security considerations</span></span>

<span data-ttu-id="48301-234">Para obter mais segurança, você pode usar [pontos de extremidade de serviço de rede Virtual](/azure/virtual-network/virtual-network-service-endpoints-overview) para proteger os recursos de serviço do Azure apenas na sua rede virtual.</span><span class="sxs-lookup"><span data-stu-id="48301-234">For additional security, you can use [Virtual Network service endpoints](/azure/virtual-network/virtual-network-service-endpoints-overview) to secure Azure service resources to only your virtual network.</span></span> <span data-ttu-id="48301-235">Isso remove totalmente o acesso público via Internet a esses recursos, permitindo o tráfego somente da sua rede virtual.</span><span class="sxs-lookup"><span data-stu-id="48301-235">This fully removes public Internet access to those resources, allowing traffic only from your virtual network.</span></span>

<span data-ttu-id="48301-236">Com essa abordagem, você cria uma VNet no Azure e, em seguida, cria pontos de extremidade de serviço privados para serviços do Azure.</span><span class="sxs-lookup"><span data-stu-id="48301-236">With this approach, you create a VNet in Azure and then create private service endpoints for Azure services.</span></span> <span data-ttu-id="48301-237">Esses serviços são então restringidos ao tráfego de rede virtual.</span><span class="sxs-lookup"><span data-stu-id="48301-237">Those services are then restricted to traffic from that virtual network.</span></span> <span data-ttu-id="48301-238">Você também pode alcançá-los a partir da sua rede local por meio de um gateway.</span><span class="sxs-lookup"><span data-stu-id="48301-238">You can also reach them from your on-premises network through a gateway.</span></span>

<span data-ttu-id="48301-239">Esteja ciente das seguintes limitações:</span><span class="sxs-lookup"><span data-stu-id="48301-239">Be aware of the following limitations:</span></span>

- <span data-ttu-id="48301-240">No momento que essa arquitetura de referência foi criada, os pontos de extremidade de serviço da VNet contam com suporte para Armazenamento do Azure e SQL Data Warehouse do Azure, mas não para o Azure Analysis Service.</span><span class="sxs-lookup"><span data-stu-id="48301-240">At the time this reference architecture was created, VNet service endpoints are supported for Azure Storage and Azure SQL Data Warehouse, but not for Azure Analysis Service.</span></span> <span data-ttu-id="48301-241">Verificar o status mais recente [aqui](https://azure.microsoft.com/updates/?product=virtual-network).</span><span class="sxs-lookup"><span data-stu-id="48301-241">Check the latest status [here](https://azure.microsoft.com/updates/?product=virtual-network).</span></span>

- <span data-ttu-id="48301-242">Se os pontos de extremidade de serviço estão habilitados para o Armazenamento do Azure, o PolyBase não pode copiar dados do armazenamento no SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="48301-242">If service endpoints are enabled for Azure Storage, PolyBase cannot copy data from Storage into SQL Data Warehouse.</span></span> <span data-ttu-id="48301-243">Há uma mitigação para esse problema.</span><span class="sxs-lookup"><span data-stu-id="48301-243">There is a mitigation for this issue.</span></span> <span data-ttu-id="48301-244">Para obter mais informações, consulte [Impacto de usar pontos de extremidade de serviço de VNet com Armazenamento do Azure](/azure/sql-database/sql-database-vnet-service-endpoint-rule-overview?toc=%2fazure%2fvirtual-network%2ftoc.json#impact-of-using-vnet-service-endpoints-with-azure-storage).</span><span class="sxs-lookup"><span data-stu-id="48301-244">For more information, see [Impact of using VNet Service Endpoints with Azure storage](/azure/sql-database/sql-database-vnet-service-endpoint-rule-overview?toc=%2fazure%2fvirtual-network%2ftoc.json#impact-of-using-vnet-service-endpoints-with-azure-storage).</span></span>

- <span data-ttu-id="48301-245">Para mover dados do local para o Armazenamento do Azure, você precisará listar permissões dos endereços IP públicos do seu local ou ExpressRoute.</span><span class="sxs-lookup"><span data-stu-id="48301-245">To move data from on-premises into Azure Storage, you will need to whitelist public IP addresses from your on-premises or ExpressRoute.</span></span> <span data-ttu-id="48301-246">Para obter detalhes, consulte [Protegendo serviços do Azure em redes virtuais](/azure/virtual-network/virtual-network-service-endpoints-overview#securing-azure-services-to-virtual-networks).</span><span class="sxs-lookup"><span data-stu-id="48301-246">For details, see [Securing Azure services to virtual networks](/azure/virtual-network/virtual-network-service-endpoints-overview#securing-azure-services-to-virtual-networks).</span></span>

- <span data-ttu-id="48301-247">Para habilitar o Analysis Services para ler dados do SQL Data Warehouse, implante uma VM do Windows para a rede virtual que contém o ponto de extremidade de serviço do SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="48301-247">To enable Analysis Services to read data from SQL Data Warehouse, deploy a Windows VM to the virtual network that contains the SQL Data Warehouse service endpoint.</span></span> <span data-ttu-id="48301-248">Instale [Gateway de Dados Local do Azure](/azure/analysis-services/analysis-services-gateway) nessa VM.</span><span class="sxs-lookup"><span data-stu-id="48301-248">Install [Azure On-premises Data Gateway](/azure/analysis-services/analysis-services-gateway) on this VM.</span></span> <span data-ttu-id="48301-249">Depois conecte seu serviço do Azure Analysis para o gateway de dados.</span><span class="sxs-lookup"><span data-stu-id="48301-249">Then connect your Azure Analysis service to the data gateway.</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="48301-250">Implantar a solução</span><span class="sxs-lookup"><span data-stu-id="48301-250">Deploy the solution</span></span>

<span data-ttu-id="48301-251">Para a implantação e execução da implementação de referência, siga as etapas em [Leia-me do GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="48301-251">To the deploy and run the reference implementation, follow the steps in the [GitHub readme][github].</span></span> <span data-ttu-id="48301-252">Ela implanta o seguinte:</span><span class="sxs-lookup"><span data-stu-id="48301-252">It deploys the following:</span></span>

- <span data-ttu-id="48301-253">Uma VM Windows para simular um servidor de banco de dados local.</span><span class="sxs-lookup"><span data-stu-id="48301-253">A Windows VM to simulate an on-premises database server.</span></span> <span data-ttu-id="48301-254">Ela inclui o SQL Server 2017 e ferramentas relacionadas, juntamente com o Power BI Desktop.</span><span class="sxs-lookup"><span data-stu-id="48301-254">It includes SQL Server 2017 and related tools, along with Power BI Desktop.</span></span>
- <span data-ttu-id="48301-255">Uma conta de armazenamento do Azure que fornece armazenamento de blobs para armazenar os dados exportados do banco de dados SQL Server.</span><span class="sxs-lookup"><span data-stu-id="48301-255">An Azure storage account that provides Blob storage to hold data exported from the SQL Server database.</span></span>
- <span data-ttu-id="48301-256">Uma instância do SQL Data Warehouse do Azure.</span><span class="sxs-lookup"><span data-stu-id="48301-256">An Azure SQL Data Warehouse instance.</span></span>
- <span data-ttu-id="48301-257">Uma instância do Azure Analysis Services.</span><span class="sxs-lookup"><span data-stu-id="48301-257">An Azure Analysis Services instance.</span></span>
- <span data-ttu-id="48301-258">O Azure Data Factory e o pipeline do Data Factory para o trabalho ELT.</span><span class="sxs-lookup"><span data-stu-id="48301-258">Azure Data Factory and the Data Factory pipeline for the ELT job.</span></span>

## <a name="related-resources"></a><span data-ttu-id="48301-259">Recursos relacionados</span><span class="sxs-lookup"><span data-stu-id="48301-259">Related resources</span></span>

<span data-ttu-id="48301-260">O ideal é examinar os seguintes [cenários de exemplo do Azure](/azure/architecture/example-scenario), que demonstram soluções específicas usando algumas das mesmas tecnologias:</span><span class="sxs-lookup"><span data-stu-id="48301-260">You may want to review the following [Azure example scenarios](/azure/architecture/example-scenario) that demonstrate specific solutions using some of the same technologies:</span></span>

- [<span data-ttu-id="48301-261">Data warehouse e análise para vendas e marketing</span><span class="sxs-lookup"><span data-stu-id="48301-261">Data warehousing and analytics for sales and marketing</span></span>](/azure/architecture/example-scenario/data/data-warehouse)
- [<span data-ttu-id="48301-262">ETL Híbrido com Azure Data Factory e SSIS local existentes</span><span class="sxs-lookup"><span data-stu-id="48301-262">Hybrid ETL with existing on-premises SSIS and Azure Data Factory</span></span>](/azure/architecture/example-scenario/data/hybrid-etl-with-adf)

<!-- links -->

[adf]: /azure/data-factory
[github]: https://github.com/mspnp/azure-data-factory-sqldw-elt-pipeline
[MergeLocation]: https://github.com/mspnp/reference-architectures/blob/master/data/enterprise_bi_sqldw_advanced/azure/sqldw_scripts/city/%5BIntegration%5D.%5BMergeLocation%5D.sql
[wwi]: /sql/sample/world-wide-importers/wide-world-importers-oltp-database
